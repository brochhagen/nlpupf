{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c6ca76",
   "metadata": {},
   "source": [
    "# Text summarization\n",
    "\n",
    "Approaches can be roughly categorized into *abstractive* and *extractive* summarization. Extractive summarization works more closely with the input given, *extracting* the most important sentence(s) from a text and making a summary out of that. Abstractive summarization tries to synthesize the text in a more holistic way, producing completely new sentences.\n",
    "\n",
    "The (extractive) summarization pipeline follows three steps\n",
    "\n",
    "1. Sentence scoring: Which sentences are the most important?\n",
    "2. Sentence selection: Which sentences out of 1 carry complementary information?\n",
    "3. Sentence reformulation: Which material can I reformulate / compress further?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Many approaches are frequency based. That is, they assume that the most important information in a text will appear more frequently in the text. Is this a reasonable assumption?\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6930e44",
   "metadata": {},
   "source": [
    "# ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "ROUGE-N is the n-gram recall between a candidate summary and a set of reference summaries. \n",
    "\n",
    "$$\\Large \\text{ROUGE-N}_{\\text{recall}} = \\frac{\\sum_{S \\in \\{ \\text{Reference Summaries} \\}}\\sum_{gram_n \\in S} \\text{Count}_{\\text{match}}(gram_n)}{\\sum_{S \\in \\{ \\text{Reference Summaries} \\}}\\sum_{gram_n \\in S} \\text{Count}(gram_n)},$$\n",
    "\n",
    "where $n$ is the length of the n-gram, and $\\text{Count}_{\\text{match}}$ is the maximum number of n-grams co-occurring in a candidate summary and a reference summary $S$.\n",
    "\n",
    "ROUGE-L is based on the longest common subsequence shared between the reference and the candidate summary (N.B.: the common words are not necessarily consecutive, just in the same sequence).\n",
    "\n",
    "$$\\Large \\text{ROUGE-L} = \\frac{LCS(S,X)}{m},$$\n",
    "\n",
    "where $m$ is the length of the reference summary. So if *Government reduces taxes next Monday* is the reference summary and our candidate is *The goverment reduces income taxes starting the following week*, we have a LCS of with \"government reduces taxes\", so 3 out of a reference summary of 5 (i.e., a ROUGE-L of 0.6).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What are the advantages and weaknesses of ROUGE-N and ROUGE-L?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b0cb2",
   "metadata": {},
   "source": [
    "# Term Frequencyâ€“Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "$$\\Large \\text{tf(t,d)} = \\frac{f_{t,d}}{\\sum_{t' \\in d}f_{t',d}},$$\n",
    "\n",
    "where $f_{t,d}$ is the raw count of a term $t$ in a document $d$.\n",
    "\n",
    "$$\\Large \\text{idf(t,D)} = \\text{log}\\frac{|D|}{\\mid \\{ d \\in D: t \\in d \\} \\mid}$$\n",
    "\n",
    "$$\\Large \\text{tf-idf(t,d,D)} = \\text{tf}(t,d) * \\text{idf}(t,D)$$\n",
    "\n",
    "Intuitively: How important a term is in a document, weighted by how frequent that word is, in general\n",
    "\n",
    "Since we have a single document to summarize, and our task is to find sentences that have \"more information\"; here \"document\" refers to a sentence. \n",
    "\n",
    "\n",
    "## TF-IDF-based extractive summarization\n",
    "The weight of each sentence is based on a sum of the TF-IDF of its words. Pick the $n$ highest scoring ones.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What are the advantages and weaknesses of TF-IDF-based summarization?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f12f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> <br>\n",
    "    <ol> 1. Download the DailyMail data from the <a href='https://github.com/abisee/cnn-dailymail'>CNN/DailyMail summarization dataset</a>. You can download a (slightly pre-processed) version directly from here (second link): <a href='https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail](https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail'>https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail](https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail</a> </ol>\n",
    "    <ol> 2. Write a script that computes the ROUGE-2 and ROUGE-L of a model's summary based on a story, benchmarked against a concatenation of its highlights</ol>\n",
    "    <ol> 3. Implement a LEAD-2 model and evaluate its ROUGE-2 and ROUGE-L on the entire DailyMail dataset</ol>\n",
    "    <ol> 4. Implement a TF-IDF-based extractive summarization model. Find the optimal $n$-highest ranking sentences that should be picked for the summary. What is its ROUGE-2 and ROUGE-L? How How does it compare to LEAD-2?</ol>\n",
    "    <ol> 5. Qualitatively diagnose main issues with your TF-IDF-based extractive summarization model.</ol>\n",
    "    <ol> 6. Repeat steps (3) - (5) on the Aligned Abstractive-Extractive Summaries's dataset <a href='http://www.cs.columbia.edu/~ouyangj/aligned-summarization-data/aligned-summarization-data.tar.gz'>http://www.cs.columbia.edu/~ouyangj/aligned-summarization-data/aligned-summarization-data.tar.gz</a></ol>\n",
    "    <ol> 7. What similarities/differences are there between the two data sets? Discuss. </ol>\n",
    " </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpupf",
   "language": "python",
   "name": "nlpupf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
