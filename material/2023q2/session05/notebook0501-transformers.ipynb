{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece2f3b5",
   "metadata": {},
   "source": [
    "# Language models\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Activity.</b> Predict the word that comes next <br>\n",
    "    <ol>1. I went to ___</ol>\n",
    "    <ol>2. Would you like another ___ </ol>\n",
    "    <ol>3. He went up the stairs ___ </ol>\n",
    "    <ol>4. I would like ___ but today I don't have time </ol>\n",
    "    <ol>5. There are ___ animals in the zoo </ol>\n",
    "    <ol>6. This is ___ last chance you'll get</ol>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Language models are *self-supervised* neural network models trained on, e.g., \n",
    "\n",
    "1. *Causal language modeling*: predict next word given previous $n$ words; e.g., (1)-(3) above)\n",
    "2. *Masked language modeling*: predict the masked word, e.g., (4)-(6))\n",
    "\n",
    "These are general tasks. They are claimed to allow large language models to learn general linguistic properties. There is a large research area devoted to studying which natural language properties language models (do not) learn from self-supervised training on large amounts of unstructured data.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What linguistic properties do you expect a model to learn from causal or masked language modeling? Which ones do you expect it to have a harder time with?\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b5392",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are models that process sequential data (language input). What makes them special, and a large reason for their success, is that they have an *attention mechanism*. We won't go into the details here, but see [Attention is all you need!](https://arxiv.org/abs/1706.03762) and [Hugging Face's tutorial on transformers](https://huggingface.co/course). Intuitively, and by contrast to other models, they have the ability to output contextualized representations. \n",
    "\n",
    "In all likelihood, you will mainly be using *pre-trained* language models (and most likely transformers). The reason is that training a large language model requires a lot of data and computing time. Nowadays, large companies are main source of new models.\n",
    "\n",
    "Using a pre-trained language model and fine-tuning it for a particular task is called *transfer learning*\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What are the advantages of using a pre-trained model? What are the disadvantages?\n",
    "</div>\n",
    "\n",
    "### Data hungry algorithms and bias\n",
    "\n",
    "In the tutorial, you came across the following example of bias induced by training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d2a3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b796a14",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What other types of biases (linguistic and cultural) may a language model inadvertedly incorporate?\n",
    "    \n",
    "    \n",
    "Identifying and mitigating bias in language models is a fast growing area in industry and academia.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpupf",
   "language": "python",
   "name": "nlpupf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
