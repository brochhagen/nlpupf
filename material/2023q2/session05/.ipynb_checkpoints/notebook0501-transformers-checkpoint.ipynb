{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece2f3b5",
   "metadata": {},
   "source": [
    "# Language models\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Activity.</b> Predict the word that comes next <br>\n",
    "    <ol>1. I went to ___</ol>\n",
    "    <ol>2. Would you like another ___ </ol>\n",
    "    <ol>3. He went up the stairs ___ </ol>\n",
    "    <ol>4. I would like ___ but today I don't have time </ol>\n",
    "    <ol>5. There are ___ animals in the zoo </ol>\n",
    "    <ol>6. This is ___ last chance you'll get</ol>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Language models are *self-supervised* nerual network models trained on, e.g., \n",
    "\n",
    "1. *Causal language modeling*: predict next word given previous $n$ words; e.g., (1)-(3) above)\n",
    "2. *Masked language modeling*: predict the masked word, e.g., (4)-(6))\n",
    "\n",
    "These are general tasks. They are claimed to allow large language models to learn general linguistic properties. There is a large research area devoted to studying which natural language properties language models (do not) learn from self-supervised training on large amounts of unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b5392",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are models that process sequential data (language input). What makes them special, and a large reason for their success, is that they have an *attention mechanism*. We won't go into the details here, but see [Attention is all you need!](https://arxiv.org/abs/1706.03762) and [Hugging Face's tutorial on transformers](https://huggingface.co/course)\n",
    "\n",
    "In all likelihood, you will mainly be using *pre-trained* language models (and most likely transformers). The reason is that training a large language model requires a lot of data and computing time. Nowadays, large companies are main source of new models.\n",
    "\n",
    "Using a pre-trained language model and fine-tuning it for a particular task is called *transfer learning*\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What are the advantages of using a pre-trained model? What are the disadvantages?\n",
    "</div>\n",
    "\n",
    "### Data hungry algorithms and bias\n",
    "\n",
    "In the tutorial, you came across the following example of bias induced by training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d2a3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b796a14",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What other types of biases (linguistic and cultural) may a language model inadvertedly incorporate?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a445c",
   "metadata": {},
   "source": [
    "# Transformers and spaCy\n",
    "\n",
    "`spaCy` has a wrapper for the `transformers` library, called `spacy-transformers`. There are some tasks that the wrapper does not support (for instance, <MASK>-prediction, since it is considered a Natural Language Generation task); but many are supported, including fine tuning.\n",
    "    \n",
    "More information can be found on spaCy's webpage and [here](https://explosion.ai/blog/spacy-transformers)\n",
    "    \n",
    "    \n",
    "### Word embeddings\n",
    "Besides fine-tuning, you can also easily access a language model's word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3dae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12119/689547268.py:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
      "/tmp/ipykernel_12119/689547268.py:14: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.27134929909014804\n",
      "salty fries <-> hamburgers 0.40727245807647705\n",
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.0\n",
      "salty fries <-> hamburgers 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12119/689547268.py:27: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
      "/tmp/ipykernel_12119/689547268.py:27: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
      "/tmp/ipykernel_12119/689547268.py:31: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))\n",
      "/tmp/ipykernel_12119/689547268.py:31: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # make sure to use larger package!\n",
    "\n",
    "\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "affc698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Apple\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "doc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")\n",
    "\n",
    "# doc[0] accesses the emmbedding of the first token = 'Apple'\n",
    "print(doc[0])  # Apple\n",
    "\n",
    "# doc[7] accesses the emmbedding of the 8th token = 'Apple'\n",
    "print(doc[7])  # Apple\n",
    "\n",
    "# they are not the same, because the embedding are context sentitive (check with cosine similarity)    \n",
    "print(doc[0].similarity(doc[7])) # 0.43365735\n",
    "\n",
    "#doc1 = nlp(u\"The labrador barked.\")\n",
    "#doc2 = nlp(u\"The labrador swam.\")\n",
    "#doc3 = nlp(u\"the labrador people live in canada.\")\n",
    "\n",
    "#for doc in [doc1, doc2, doc3]:\n",
    "#    labrador = doc[1]\n",
    "#    dog = nlp(u\"dog\")\n",
    "#    print(labrador.similarity(dog))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a171cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> Use spaCy's en_core_web_sm's word embeddings to answer the following questions\n",
    "    1. \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpupf",
   "language": "python",
   "name": "nlpupf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
