{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672a445c",
   "metadata": {},
   "source": [
    "# Transformers and spaCy\n",
    "\n",
    "`spaCy` has a wrapper for the `transformers` library, called `spacy-transformers`. There are some tasks that the wrapper does not support (for instance, <MASK>-prediction, since it is considered a Natural Language Generation task); but many are supported, including fine tuning.\n",
    "    \n",
    "More information can be found on spaCy's webpage and [here](https://explosion.ai/blog/spacy-transformers)\n",
    "    \n",
    "    \n",
    "# Word embeddings\n",
    "Typically, real-valued representations of words learned from text (often: through a language modelling task). You can regard word embeddings as one of the main components of linguistic knowledge they acquired after training.    \n",
    "    \n",
    "    \n",
    "You can easily access a language model's word embeddings\n",
    "    \n",
    "### Static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3dae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First thirty dimensions of the vector for \"Apple\" in doc1:\n",
      " [-1.6859    2.9122    2.5477    3.7146    2.4753    0.98192  -4.0277\n",
      "  4.694    -4.7332   -4.226     0.060411 -1.8773   -5.0201    1.8334\n",
      "  0.47305  -3.288     6.3601   -1.6796    1.6396   -2.1696    3.5784\n",
      "  5.1092   -2.0516   -0.57622  -6.6237    0.46293  -1.4119    0.67972\n",
      " -0.35862   1.2775  ]\n",
      "\n",
      " First thirty dimensions of the vector for \"Apple\" in doc1:\n",
      " [-1.6859    2.9122    2.5477    3.7146    2.4753    0.98192  -4.0277\n",
      "  4.694    -4.7332   -4.226     0.060411 -1.8773   -5.0201    1.8334\n",
      "  0.47305  -3.288     6.3601   -1.6796    1.6396   -2.1696    3.5784\n",
      "  5.1092   -2.0516   -0.57622  -6.6237    0.46293  -1.4119    0.67972\n",
      " -0.35862   1.2775  ]\n",
      "\n",
      " Number of dimensions/shape of the word embeddings in en_core_web_md:\n",
      " (300,)\n",
      "\n",
      " Are the two vectors identical? True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md') #spaCy sm models do not ship with word embeddings\n",
    "\n",
    "doc1 = nlp('Apple shares rose on the news.')\n",
    "doc2 = nlp('Apple pie is delicious.')\n",
    "\n",
    "print('First thirty dimensions of the vector for \"Apple\" in doc1:\\n',\n",
    "      doc1[0].vector[:30])\n",
    "print('\\n First thirty dimensions of the vector for \"Apple\" in doc1:\\n',\n",
    "      doc2[0].vector[:30])\n",
    "print('\\n Number of dimensions/shape of the word embeddings in en_core_web_md:\\n', doc1[0].vector.shape)\n",
    "\n",
    "print('\\n Are the two vectors identical?', (doc1[0].vector == doc2[0].vector).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e10018",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What do you expect to be the similarity ranking of the words \"cat\", \"snake\", \"car\" and \"random\" with respect to the word \"dog\"?</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> Use spaCy's en_core_web_md's word embeddings to retrieve the similarities of the words above.\n",
    "<div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1924d443",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> Use spaCy's en_core_web_md's word embeddings to find:<br>\n",
    "    \n",
    "  1. The word with the smallest cosine similarity to the word \"apple\"; \n",
    "  2. The largest cosine similarity to the word \"apple\";\n",
    "  3. The second largest cosine similarity to the word \"apple\".\n",
    "</div>\n",
    "\n",
    "***    \n",
    "\n",
    "SpaCy's `similarity`-function is not only restricted to tokens, but can also be applied to documents and spans. Their representations are the average of the token vectors that are found within the document/span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef7722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.691649353055761\n",
      "salty fries <-> hamburgers 0.6938489675521851\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf78d3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Is the \"mean embedding\" of the tokens within a document/span a reasonable representation for a document/span? What are its disadvantages? Can you think of alternatives?</div>\n",
    "\n",
    "See __[Nouns are Vectors, Adjectives are Matrices](https://aclanthology.org/D10-1115.pdf)__, i.a., for compositional vectorial representations for static vectors.\n",
    "\n",
    "***\n",
    "\n",
    "### Contextualized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affc698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1093, in _get_module\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.audio_spectrogram_transformer.configuration_audio_spectrogram_transformer'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/h0/fbc05cq92w5bw3_kdg_h4rpw0000gn/T/ipykernel_62631/2627975509.py\", line 1, in <module>\n",
      "    nlp_trf = spacy.load('en_core_web_trf')\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/__init__.py\", line 54, in load\n",
      "    return util.load_model(\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/util.py\", line 432, in load_model\n",
      "    return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/util.py\", line 468, in load_model_from_package\n",
      "    return cls.load(vocab=vocab, disable=disable, enable=enable, exclude=exclude, config=config)  # type: ignore[attr-defined]\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/en_core_web_trf/__init__.py\", line 10, in load\n",
      "    return load_model_from_init_py(__file__, **overrides)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/util.py\", line 649, in load_model_from_init_py\n",
      "    return load_model_from_path(\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/util.py\", line 514, in load_model_from_path\n",
      "    return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/language.py\", line 2140, in from_disk\n",
      "    util.from_disk(path, deserializers, exclude)  # type: ignore[arg-type]\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/util.py\", line 1352, in from_disk\n",
      "    reader(path / key)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/language.py\", line 2134, in <lambda>\n",
      "    deserializers[name] = lambda p, proc=proc: proc.from_disk(  # type: ignore[misc]\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy_transformers/pipeline_component.py\", line 419, in from_disk\n",
      "    util.from_disk(path, deserialize, exclude)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy/util.py\", line 1352, in from_disk\n",
      "    reader(path / key)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy_transformers/pipeline_component.py\", line 393, in load_model\n",
      "    self.model.from_bytes(mfile.read())\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/thinc/model.py\", line 619, in from_bytes\n",
      "    return self.from_dict(msg)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/thinc/model.py\", line 657, in from_dict\n",
      "    node.shims[i].from_bytes(shim_bytes)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/spacy_transformers/layers/hf_shim.py\", line 106, in from_bytes\n",
      "    transformer = self.model_cls.from_config(config)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 408, in from_config\n",
      "    elif type(config) in cls._model_mapping.keys():\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 607, in keys\n",
      "    (\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 608, in <listcomp>\n",
      "    self._load_attr_from_module(key, self._config_mapping[key]),\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 604, in _load_attr_from_module\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 553, in getattribute_from_module\n",
      "    self._reverse_config_mapping = {v: k for k, v in config_mapping.items()}\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1083, in __getattr__\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1095, in _get_module\n",
      "RuntimeError: Failed to import transformers.models.audio_spectrogram_transformer.configuration_audio_spectrogram_transformer because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.audio_spectrogram_transformer.configuration_audio_spectrogram_transformer'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/anaconda3/envs/nlpupf/lib/python3.9/site-packages/executing/executing.py\", line 168, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "nlp_trf = spacy.load('en_core_web_trf')\n",
    "\n",
    "doc1 = nlp_trf(\"Apple shares rose on the news.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the doc1-object\n",
    "doc1.__len__() #why 7 and not 6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"tensors\"-attribute of a TransformerData object contains \n",
    "#a Python list with vector representations generated by the transformer\n",
    "#for individual tokens and the entire doc1 object.\n",
    "\n",
    "# Check the shape of the first item in the list. \n",
    "#This is the output for individual tokens\n",
    "doc1._.trf_data.tensors[0].shape #1 batch of 9 vectors with 768 dimensions each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c31ae",
   "metadata": {},
   "source": [
    "You can think of a **tensor** as a bundle of numerical objects; or as a multi-dimensional array; or as a higher-dimensional matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the second item in the list. \n",
    "#This is the output for the entire document\n",
    "doc1._.trf_data.tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ten dimensions of the tensor\n",
    "doc1._.trf_data.tensors[0][0][:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45827b",
   "metadata": {},
   "source": [
    "This looks very similar to the embeddings we had before; only that we're now talking about tensors and accessing them is a little more cumbersome. \n",
    "\n",
    "However, we had established that there were **7** tokens in `doc1` but we now have **9** embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the Transformer tokens under the key 'input_texts'\n",
    "doc1._.trf_data.tokens['input_texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ca309",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_split = nlp_trf('Do you really need a representation for words like miscommunication or wordly?')\n",
    "doc_split._.trf_data.tokens['input_texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292f8d9",
   "metadata": {},
   "source": [
    "Sub-word representations are a standard solution for out of vocabulary representations. See also __[fastText](https://fasttext.cc/)__ for static sub-word representations.\n",
    "\n",
    "Since tokens may not correspond 1:1 to individual vectors, we first need to align them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the vectors corresponding to token 9 (miscommunication)\n",
    "doc_split[9], doc_split._.trf_data.align[9].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1[0], doc1._.trf_data.align[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9f1fd",
   "metadata": {},
   "source": [
    "### Adding a class to the spaCy pipeline\n",
    "\n",
    "There is no good native way to compare contextualized word embeddings in spaCy. However, one of the mean advantages of this package is how easy it is to modify or add components to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77baa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class from: https://applied-language-technology.mooc.fi\n",
    "\n",
    "# Import the Language object under the 'language' module in spaCy,\n",
    "# and NumPy for calculating cosine similarity.\n",
    "from spacy.language import Language\n",
    "import numpy as np\n",
    "\n",
    "# We use the @ character to register the following Class definition\n",
    "# with spaCy under the name 'tensor2attr'.\n",
    "@Language.factory('tensor2attr')\n",
    "\n",
    "# We begin by declaring the class name: Tensor2Attr. The name is \n",
    "# declared using 'class', followed by the name and a colon.\n",
    "class Tensor2Attr:\n",
    "    \n",
    "    # We continue by defining the first method of the class, \n",
    "    # __init__(), which is called when this class is used for \n",
    "    # creating a Python object. Custom components in spaCy \n",
    "    # require passing two variables to the __init__() method:\n",
    "    # 'name' and 'nlp'. The variable 'self' refers to any\n",
    "    # object created using this class!\n",
    "    def __init__(self, name, nlp):\n",
    "        \n",
    "        # We do not really do anything with this class, so we\n",
    "        # simply move on using 'pass' when the object is created.\n",
    "        pass\n",
    "\n",
    "    # The __call__() method is called whenever some other object\n",
    "    # is passed to an object representing this class. Since we know\n",
    "    # that the class is a part of the spaCy pipeline, we already know\n",
    "    # that it will receive Doc objects from the preceding layers.\n",
    "    # We use the variable 'doc' to refer to any object received.\n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        # When an object is received, the class will instantly pass\n",
    "        # the object forward to the 'add_attributes' method. The\n",
    "        # reference to self informs Python that the method belongs\n",
    "        # to this class.\n",
    "        self.add_attributes(doc)\n",
    "        \n",
    "        # After the 'add_attributes' method finishes, the __call__\n",
    "        # method returns the object.\n",
    "        return doc\n",
    "    \n",
    "    # Next, we define the 'add_attributes' method that will modify\n",
    "    # the incoming Doc object by calling a series of methods.\n",
    "    def add_attributes(self, doc):\n",
    "        \n",
    "        # spaCy Doc objects have an attribute named 'user_hooks',\n",
    "        # which allows customising the default attributes of a \n",
    "        # Doc object, such as 'vector'. We use the 'user_hooks'\n",
    "        # attribute to replace the attribute 'vector' with the \n",
    "        # Transformer output, which is retrieved using the \n",
    "        # 'doc_tensor' method defined below.\n",
    "        doc.user_hooks['vector'] = self.doc_tensor\n",
    "        \n",
    "        # We then perform the same for both Spans and Tokens that\n",
    "        # are contained within the Doc object.\n",
    "        doc.user_span_hooks['vector'] = self.span_tensor\n",
    "        doc.user_token_hooks['vector'] = self.token_tensor\n",
    "        \n",
    "        # We also replace the 'similarity' method, because the \n",
    "        # default 'similarity' method looks at the default 'vector'\n",
    "        # attribute, which is empty! We must first replace the\n",
    "        # vectors using the 'user_hooks' attribute.\n",
    "        doc.user_hooks['similarity'] = self.get_similarity\n",
    "        doc.user_span_hooks['similarity'] = self.get_similarity\n",
    "        doc.user_token_hooks['similarity'] = self.get_similarity\n",
    "    \n",
    "    # Define a method that takes a Doc object as input and returns \n",
    "    # Transformer output for the entire Doc.\n",
    "    def doc_tensor(self, doc):\n",
    "        \n",
    "        # Return Transformer output for the entire Doc. As noted\n",
    "        # above, this is the last item under the attribute 'tensor'.\n",
    "        # Average the output along axis 0 to handle batched outputs.\n",
    "        return doc._.trf_data.tensors[-1].mean(axis=0)\n",
    "    \n",
    "    # Define a method that takes a Span as input and returns the Transformer \n",
    "    # output.\n",
    "    def span_tensor(self, span):\n",
    "        \n",
    "        # Get alignment information for Span. This is achieved by using\n",
    "        # the 'doc' attribute of Span that refers to the Doc that contains\n",
    "        # this Span. We then use the 'start' and 'end' attributes of a Span\n",
    "        # to retrieve the alignment information. Finally, we flatten the\n",
    "        # resulting array to use it for indexing.\n",
    "        tensor_ix = span.doc._.trf_data.align[span.start: span.end].data.flatten()\n",
    "        \n",
    "        # Fetch Transformer output shape from the final dimension of the output.\n",
    "        # We do this here to maintain compatibility with different Transformers,\n",
    "        # which may output tensors of different shape.\n",
    "        out_dim = span.doc._.trf_data.tensors[0].shape[-1]\n",
    "        \n",
    "        # Get Token tensors under tensors[0]. Reshape batched outputs so that\n",
    "        # each \"row\" in the matrix corresponds to a single token. This is needed\n",
    "        # for matching alignment information under 'tensor_ix' to the Transformer\n",
    "        # output.\n",
    "        tensor = span.doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n",
    "        \n",
    "        # Average vectors along axis 0 (\"columns\"). This yields a 768-dimensional\n",
    "        # vector for each spaCy Span.\n",
    "        return tensor.mean(axis=0)\n",
    "    \n",
    "    # Define a function that takes a Token as input and returns the Transformer\n",
    "    # output.\n",
    "    def token_tensor(self, token):\n",
    "        \n",
    "        # Get alignment information for Token; flatten array for indexing.\n",
    "        # Again, we use the 'doc' attribute of a Token to get the parent Doc,\n",
    "        # which contains the Transformer output.\n",
    "        tensor_ix = token.doc._.trf_data.align[token.i].data.flatten()\n",
    "        \n",
    "        # Fetch Transformer output shape from the final dimension of the output.\n",
    "        # We do this here to maintain compatibility with different Transformers,\n",
    "        # which may output tensors of different shape.\n",
    "        out_dim = token.doc._.trf_data.tensors[0].shape[-1]\n",
    "        \n",
    "        # Get Token tensors under tensors[0]. Reshape batched outputs so that\n",
    "        # each \"row\" in the matrix corresponds to a single token. This is needed\n",
    "        # for matching alignment information under 'tensor_ix' to the Transformer\n",
    "        # output.\n",
    "        tensor = token.doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n",
    "\n",
    "        # Average vectors along axis 0 (columns). This yields a 768-dimensional\n",
    "        # vector for each spaCy Token.\n",
    "        return tensor.mean(axis=0)\n",
    "    \n",
    "    # Define a function for calculating cosine similarity between vectors\n",
    "    def get_similarity(self, doc1, doc2):\n",
    "        \n",
    "        # Calculate and return cosine similarity\n",
    "        return np.dot(doc1.vector, doc2.vector) / (doc1.vector_norm * doc2.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the component named 'tensor2attr', which we registered using the\n",
    "# @Language decorator and its 'factory' method to the pipeline.\n",
    "nlp_trf.add_pipe('tensor2attr')\n",
    "\n",
    "# Call the 'pipeline' attribute to examine the pipeline\n",
    "nlp_trf.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60206cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp_trf(\"Apple shares rose on the news.\")\n",
    "doc2 = nlp_trf('I really enjoyed this delicious apple.')\n",
    "\n",
    "# doc1[0] accesses the emmbedding of the first token = 'Apple'\n",
    "print(doc1[0])  # Apple\n",
    "\n",
    "# doc2[5] accesses the emmbedding of the 6th token = 'apple'\n",
    "print(doc2[5])  # apple\n",
    "\n",
    "print(doc1[0].similarity(doc2[5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f57c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1s = nlp(\"Apple shares rose on the news.\")\n",
    "doc2s = nlp('I really enjoyed this delicious apple.')\n",
    "\n",
    "# doc1[0] accesses the emmbedding of the first token = 'Apple'\n",
    "print(doc1s[0])  # Apple\n",
    "\n",
    "# doc2[5] accesses the emmbedding of the 6th token = 'apple'\n",
    "print(doc2s[5])  # apple\n",
    "\n",
    "print(doc1s[0].similarity(doc2s[5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp_trf('The new iPhone by Apple is expected by September 2024')\n",
    "doc4 = nlp_trf('I liked the red apple the most')\n",
    "\n",
    "print(doc3[4])\n",
    "\n",
    "# doc2[5] accesses the emmbedding of the 6th token = 'apple'\n",
    "print(doc4[4])  # apple\n",
    "\n",
    "print(doc1[0].similarity(doc3[4])) \n",
    "print(doc2[5].similarity(doc4[4])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a171cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> Use spaCy's en_core_web_md's word embeddings to answer the following questions\n",
    "    \n",
    "  1. What is the average similarity between a noun and a determiner?\n",
    "  2. What is the average similarity between a noun and a verb?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> Use spaCy's en_core_web_trf to answer the following questions\n",
    "    \n",
    "  1. What is the average similarity between a noun and a determiner?\n",
    "  2. What is the average similarity between a noun and a verb?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpupf",
   "language": "python",
   "name": "nlpupf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
