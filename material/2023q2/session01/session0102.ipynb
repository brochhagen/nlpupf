{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9831a64",
   "metadata": {},
   "source": [
    "Open in colab: https://colab.research.google.com/github/brochhagen/nlpupf/blob/main/material/2023q2/session01/session0102.ipynb\n",
    "\n",
    "\n",
    "# Handling text (Session 01.02)\n",
    "\n",
    "Main contents covered:\n",
    "\n",
    "  * Normalization\n",
    "  * Tokenization\n",
    "  * Type/token distinction\n",
    "  * Zipf's Law of Abbreviation\n",
    "  \n",
    "To distill information from textual data, chances are that you need to pre-process it first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b84ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This snippet reads in a text from a URL ###\n",
    "\n",
    "import urllib.request #for opening/reading URLs\n",
    "book_url = 'https://www.gutenberg.org/cache/epub/1727/pg1727.txt' #URL of book we want to read in\n",
    "\n",
    "book_text = urllib.request.urlopen(book_url) #open URL as \"book_text\"\n",
    "book_text = book_text.read()                 #returns all bytes from \"book_text\" \n",
    "book_text = book_text.decode(\"utf-8\")        #decode as UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46c92a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> This is a pretty typical situation. You start with a package/module that does some pre-processing, but now you need to figure out the type of the output it gave. How do you figure out what \"book_text\" is?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf38289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(book_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22394be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5041b8c",
   "metadata": {},
   "source": [
    "# Pre-processing \n",
    "\n",
    "Let's start with the question how many times each word appears in the text.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Minimally, what do we need to do with \"book_text\" before answering this question?</div>\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized_book = book_text.split(' ') #list with elements of \"book_text\", split at the specified string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57881066",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_tokenized_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ace7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized_book[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(word_tokenized_book))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e11939",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Can you think of other units of analysis than word-level tokenization?</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Can we now answer the question how many times each word appears in the text?</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freq_book = Counter(word_tokenized_book)\n",
    "freq_book.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233bd52",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966b4a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_tokenized_normed_book = [w.lower() for w in word_tokenized_book]\n",
    "freq_book = Counter(word_tokenized_normed_book)\n",
    "freq_book.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b76a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Look at the tokenized book. What other normalization processes can you think of?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd9441",
   "metadata": {},
   "source": [
    "## Types vs. tokens\n",
    "\n",
    "*Types* are the distinct units in a corpus (e.g., distinct words). *Tokens* are each occurence of a unit in a corpus (e.g., each word). So the number of *types* in a corpus will be equal or less than the number of *tokens*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> How many word types are there in The Odyssey? How many word tokens?</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb5169",
   "metadata": {},
   "source": [
    "# The NLP pipeline & common tasks\n",
    "\n",
    "![](spacy-pipeline.svg \"Spacy pipeline\")\n",
    "<br>\n",
    "<div style=\"text-align: right\"> https://spacy.io/usage/processing-pipelines </div>\n",
    "\n",
    "\n",
    "### Some common tasks\n",
    "\n",
    "  * Label prediction\n",
    "  * Q&A\n",
    "  * Information extraction\n",
    "  * Summarization\n",
    "  * Machine translation\n",
    "  * Image captioning\n",
    "  * Text generation\n",
    "  * ...\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b6955",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "State of the art NLP module, interfaces with deep learning techniques and symbolic ones.\n",
    "\n",
    "See __[https://spacy.io/](https://spacy.io/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb35d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp(book_text)\n",
    "# all tokens that arent stop words or punctuations\n",
    "words = [token.text\n",
    "         for token in doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# noun tokens that arent stop words or punctuations\n",
    "nouns = [token.text\n",
    "         for token in doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "\n",
    "# five most common noun tokens\n",
    "noun_freq = Counter(nouns)\n",
    "common_nouns = noun_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a73925",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4760c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f962253",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> You can introduce spaCy to the group next week for participation credit.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eaae8b",
   "metadata": {},
   "source": [
    "## Excursion: Zipf's Law of Abbreviation\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> What is Zipf's (Power) Law?</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> What is Zipf's Law of Abbreviation?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "df = pd.DataFrame.from_records(list(dict(Counter(words)).items()), columns=['word','frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['frequency'], ascending=False)\n",
    "df['rank'] = list(range(1, len(df) + 1))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a683d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"rank\", y=\"frequency\", data=df);\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first row from data frame\n",
    "df.drop(index=df.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "\n",
    "# Drop first row from data frame\n",
    "df.drop(index=df.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "\n",
    "# re-do rank with new elements\n",
    "df['rank'] = list(range(1, len(df) + 1)) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f571fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"rank\", y=\"frequency\", data=df);\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876d0b6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Activity.</b> Find a text that interests you.\n",
    "\n",
    "  1. Load it\n",
    "  2. Tokenize it  (what is the unit of analysis you are interested in?)\n",
    "  3. Normalize it (what pre-processing is required?)\n",
    "  4. Diagnose it  (what is still outstanding?)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fec45",
   "metadata": {},
   "source": [
    "If you cannot think of anything, here's a collection of tweets by Donald Trump. Answer the following questions about it:\n",
    "\n",
    "  * What is the most frequent word tweeted by D. Trump?\n",
    "      \n",
    "  * What city does he mention most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32154bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump = pd.read_csv('https://raw.githubusercontent.com/ecdedios/into-heart-of-darkness/master/trump_20200530_clean.csv')\n",
    "df_trump = df_trump.head(1000) #just the first 1000 tweets for ease of processing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpupf",
   "language": "python",
   "name": "nlpupf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
