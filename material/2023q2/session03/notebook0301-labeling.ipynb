{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece2f3b5",
   "metadata": {},
   "source": [
    "# Labelling\n",
    "\n",
    "A large family of NLP tasks fall under the category of *labelling tasks** \n",
    "\n",
    "  * Part-of-speech tagging\n",
    "  * Named Entity Recognition\n",
    "  * Sentiment analysis\n",
    "  * ...\n",
    "  \n",
    "## PoS tagging\n",
    "\n",
    "Assign a part of speech label to each token. SpaCy's labeling scheme: [https://spacy.io/models/en](https://spacy.io/models/en) \n",
    "\n",
    "PoS tagging is often evaluated in terms of accuracy. For $y_n$ true labels, corresponding $\\hat{y_n}$ predicted labels, and $\\delta(x,y) = 1$ iff $x = y$ and otherwise 0: \n",
    "\n",
    "$$ \\frac{\\sum \\delta(y_i,\\hat{y_i})}{\\mid Y \\mid} $$\n",
    "\n",
    "In other words, accuracy is the fraction of correctly predicted PoS tags, divided by the total number of tags to be predicted\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Manually tag the following three sentences. Use spaCy's English labelling scheme.</div>\n",
    "\n",
    "  1. It's no use going back to yesterday, because I was a different person then.\n",
    "  2. The best way to explain it is to do it.\n",
    "  3. Never let anyone drive you crazy; it is nearby anyway and the walk is good for you.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Compare your manual tags with those from spaCy. What is its accuracy?</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Write a function that takes a sentence, a gold annotation, and returns the accuracy of spaCy on that sentence</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e98245",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Assign a label (and possibly subcategorize) to entities and non-entities. These could be individual tokens or larger spans.\n",
    "\n",
    "Named Entity Recognition is often evaluated in terms of precision (fraction of true positives out of total entities recognized), recall (fraction of true positives out of total entities in data) and F1-score (harmonic mean of precision and recall).\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Manually tag the following fragment from Alice in Wonderland.</div>\n",
    "\n",
    "  1. “Who are you?” said the Caterpillar. This was not an encouraging opening for a conversation. Alice replied, rather shyly, “I—I hardly know, Sir, just at present—at least I know who I was when I got up this morning, but I think I must have been changed several times since then.” “What do you mean by that?” said the Caterpillar, sternly. “Explain yourself!” “I can’t explain myself, I’m afraid, Sir,” said Alice, “because I am not myself, you see.” \n",
    "  \n",
    "<div class=\"alert alert-block alert-success\"> Compare your manual tags with those from spaCy. What is spaCy's F1-score?</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Write a function that takes a sentence, a gold annotation, and returns the F1-score of spaCy on that sentence</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
