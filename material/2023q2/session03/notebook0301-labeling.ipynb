{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece2f3b5",
   "metadata": {},
   "source": [
    "# Labelling\n",
    "\n",
    "A large family of NLP tasks fall under the category of *labelling tasks** \n",
    "\n",
    "  * Part-of-speech tagging\n",
    "  * Named Entity Recognition\n",
    "  * Sentiment analysis\n",
    "  * ...\n",
    "  \n",
    "## PoS tagging\n",
    "\n",
    "Assign a part of speech label to each token. \n",
    "\n",
    "SpaCy's labeling scheme is specified for each __[model of a language](https://spacy.io/models/en)__. Use `spacy.explain()` to get an explanation for a label. For example `spacy.explain('NN')`.\n",
    "\n",
    "PoS tagging is often evaluated in terms of accuracy. For $y_n$ true labels, corresponding $\\hat{y_n}$ predicted labels, and $\\delta(x,y) = 1$ iff $x = y$ and otherwise 0: \n",
    "\n",
    "$$ \\frac{\\sum \\delta(y_i,\\hat{y_i})}{\\mid Y \\mid} $$\n",
    "\n",
    "In other words, accuracy is the fraction of correctly predicted PoS tags, divided by the total number of tags to be predicted\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> 1. Manually tag the following three sentences. Use spaCy's English labelling scheme.</div>\n",
    "\n",
    "  1. It's no use going back to yesterday, because I was a different person then.\n",
    "  2. The best way to explain it is to do it.\n",
    "  3. Never let anyone drive you crazy; it is nearby anyway and the walk is good for you.\n",
    "  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> 2. Compare your manual tags with those from spaCy. What is its accuracy?</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> 3. Write a function that takes a sentence, a gold annotation, and returns the accuracy of spaCy on that sentence</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e98245",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Assign a label (and possibly subcategorize) to entities and non-entities. These could be individual tokens or larger spans.\n",
    "\n",
    "Named Entity Recognition is often evaluated in terms of precision (fraction of true positives out of total entities recognized), recall (fraction of true positives out of total entities in data) and F1-score (harmonic mean of precision and recall).\n",
    "\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$F_1 = 2\\times\\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> 1. Manually tag the following fragment from Alice in Wonderland using spaCy's English labelling scheme</div>\n",
    "\n",
    "  1. “Who are you?” said the Caterpillar. This was not an encouraging opening for a conversation. Alice replied, rather shyly, “I—I hardly know, Sir, just at present—at least I know who I was when I got up this morning, but I think I must have been changed several times since then.” “What do you mean by that?” said the Caterpillar, sternly. “Explain yourself!” “I can’t explain myself, I’m afraid, Sir,” said Alice, “because I am not myself, you see.” \n",
    "  \n",
    "<div class=\"alert alert-block alert-success\"> 2. Compare your manual tags with those from spaCy. What is spaCy's F1-score?</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> 3. Write a function that takes a sentence, a gold annotation, and returns the F1-score of spaCy on that sentence</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc5fb1",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Other evaluation measures\n",
    "\n",
    "![](bclass.png)\n",
    "\n",
    "There are many different ways to evaluate a model. *Precision*, *recall*, *F1* and *accuracy* are widely used, but the main question you should ask yourself if the method you are using faithfully quantifies performance along the task you set out to test.\n",
    "\n",
    "* Central tendencies: arithmetic mean, mode, median, harmonic mean\n",
    "* Predictive accumen: accuracy, $R^2$, expected log predictive density, precision, recall\n",
    "* Dispersion: variance, standard deviation\n",
    "\n",
    "\n",
    "Not only the measure itself is important, but also the data you evaluate it on:\n",
    "\n",
    "* Train/test splits\n",
    "* Leave-one-out\n",
    "* Leave-k-out\n",
    "\n",
    "And also the predictions you evaluate:\n",
    "* Categorical predictions\n",
    "* Probablistic preidctions\n",
    "\n",
    "\n",
    "### ROC curves\n",
    "\n",
    "**R**eceiver **o**perating **c**haracterisitic (ROC) curves plot the true positive rate against the false positive rate. You can think of it as a visualization of how well your model is doing at different decision thresholds (think: $p$ at which you classify an entity as belong to a class or not).\n",
    "\n",
    "![](ROC.jpeg)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <bf>Discussion.</bf> Which model is better? The red one (left plot) or the blue one (right plot)? Why?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc5d11",
   "metadata": {},
   "source": [
    "\n",
    "But computing all possible points would be cumbersome. One way to optimize this process is to approximate and compute the the **a**rea **u**nder the (ROC) **c**urve (AUC). The AUC is classification-threshold-invariant: It is an aggregate of the performance you can expect across all possible thresholds. It is the probability that your model ranks a randomly chosen positive higher than a randomly chosen negative one. In other words, when given one random positive and negative, the area under the curve is the probability that the model will be able to tell them apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a11ed",
   "metadata": {},
   "source": [
    "The AUC is in [0,1]. If $\\text{AUC} = 0.0$ then your model is always wrong; if it is $1$ it always classifies correctly. If it is $\\geq 0.5$ then it's doing better than chance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
