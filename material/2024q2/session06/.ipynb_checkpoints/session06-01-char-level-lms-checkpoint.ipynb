{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5b201c",
   "metadata": {},
   "source": [
    "# Session 06: (Character-level) language modelling\n",
    "\n",
    "After having briefly used LLMs (session 04) and having looked at how a simpler neural network can learn (session 05), we return to LLMs with a greater understanding for their inside workings; review main concepts; and put them to practice.\n",
    "\n",
    "By this stage, you should be able to read the diagram of a neural network. For instance, an abstract diagram like this one:\n",
    "\n",
    "![](upf_cl_logo.png)\n",
    "\n",
    "Or a more concrete one like [Figure 1 from Bengio et al. 2023](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "![Bengio et al](bengioetal.png)\n",
    "\n",
    "\n",
    "### Review: concepts and architecture\n",
    "\n",
    "Some concepts/issues covered so far:\n",
    "\n",
    "* Bigram model\n",
    "* Language modeling\n",
    "* Character level language modeling\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> In which sense is tokenization relevant to language modeling? </div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What are the advantages and limitations of a BIGRAM model? </div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Explain.</b> What is <i>one hot encoding</i>? What is it used for? </div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">  <b>Explain.</b> What is the softmax activation function used for? In which layer would you normally encounter it in a NN? </div>\n",
    "\n",
    "See also [Franke & Degen's: The softmax function: Properties, motivation, and interpretation](https://osf.io/preprints/psyarxiv/vsw47)\n",
    "\n",
    "\n",
    "### Review: Training\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">  <b>Explain.</b> What can the (negative log) likelihood be used for? Do you agree that it is a good measure of model performance when predicting token sequences? </div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">  <b>Explain.</b> Explain how neural networks are trained at a high-level. In your explanation, go through the following concepts: *loss*, *learning rate*, *(mini)batch*, *train/test/validation-split*. </div>\n",
    "\n",
    "### Review: Hyper-parameters\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">  <b>Task.</b>  Tune the hyperparameters of the model to beat A. Karpathy's validation loss. Here is the code of the model: <a href='https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing'>in colab</a> or <a href='https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb'>github</a></div>. \n",
    "\n",
    "### All together now\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">  <b>Task.</b> Train a first character-level language model to generate names in your language of choice(★). Assess your model's quality. Feel free to use A. Karpathy's code as a template (recommended).\n",
    "\n",
    "(★) If you pick English: Do something else than human names (e.g., Company names; animal names; city names; and so on)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class='alert alert-block alert-info'> In the next hand-in exercise you need to also do the following:\n",
    "    <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">  <b>Task.</b> Pick a second language. Build a second character-level model and assess model quality. Then assess your first model on the task of generating names for the second language, and vice-versa.</div>\n",
    "    \n",
    "What do you expect will happen? \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpupf",
   "language": "python",
   "name": "nlpupf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
