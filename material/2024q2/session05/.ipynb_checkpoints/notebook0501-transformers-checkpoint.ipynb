{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece2f3b5",
   "metadata": {},
   "source": [
    "# Language models\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Activity.</b> Predict the word that comes next <br>\n",
    "    <ol>1. I went to ___</ol>\n",
    "    <ol>2. Would you like another ___ </ol>\n",
    "    <ol>3. He went up the stairs ___ </ol>\n",
    "    <ol>4. I would like ___ but today I don't have time </ol>\n",
    "    <ol>5. There are ___ animals in the zoo </ol>\n",
    "    <ol>6. This is ___ last chance you'll get</ol>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Language models are *self-supervised* neural network models trained on, e.g., \n",
    "\n",
    "1. *Causal language modeling*: predict next word given previous $n$ words; e.g., (1)-(3) above)\n",
    "2. *Masked language modeling*: predict the masked word, e.g., (4)-(6))\n",
    "\n",
    "These are general tasks. They are claimed to allow large language models to learn general linguistic properties. There is a large research area devoted to studying which natural language properties language models (do not) learn from self-supervised training on large amounts of unstructured data.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What linguistic properties do you expect a model to learn from causal or masked language modeling? Which ones do you expect it to have a harder time with?\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b5392",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are models that process sequential data (language input). What makes them special, and a large reason for their success, is that they have an *attention mechanism*. We won't go into the details here, but see [Attention is all you need!](https://arxiv.org/abs/1706.03762) and [Hugging Face's tutorial on transformers](https://huggingface.co/course). Intuitively, and by contrast to other models, they have the ability to output contextualized representations. \n",
    "\n",
    "In all likelihood, you will mainly be using *pre-trained* language models (and most likely transformers). The reason is that training a large language model requires a lot of data and computing time. Nowadays, large companies are main source of new models.\n",
    "\n",
    "Using a pre-trained language model and fine-tuning it for a particular task is called *transfer learning*\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What are the advantages of using a pre-trained model? What are the disadvantages?\n",
    "</div>\n",
    "\n",
    "### Data hungry algorithms and bias\n",
    "\n",
    "In the tutorial, you came across the following example of bias induced by training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d2a3d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h0/fbc05cq92w5bw3_kdg_h4rpw0000gn/T/ipykernel_53179/498449479.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0munmasker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fill-mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This man works as a [MASK].\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_str\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b796a14",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> What other types of biases (linguistic and cultural) may a language model inadvertedly incorporate?\n",
    "    \n",
    "    \n",
    "Identifying and mitigating bias in language models is a fast growing area in industry and academia.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21a5a0",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "# The rise of LLMs\n",
    "\n",
    "Starting with ELMO (and before that, word2vec and gloVe), there has been a constant stream of new large language models. To name a few: BERT, GPT, GPT2, RoBERTa, and recently, [ChatGPT](https://openai.com/blog/chatgpt/). These models have spawned whole new fields (e.g., \"BERTology\") and made their way into more traditional linguistics & cognitive science research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpclass",
   "language": "python",
   "name": "nlpclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
