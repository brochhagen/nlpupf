---
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---
# NLP session 05: Preparation for next session
 * Watch the second and third lectures of [Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html).
 
### Recommended

 * Read Yoav Goldberg's [A Primer on Neural Network Models for Natural Language Processing](https://arxiv.org/abs/1510.00726), at least until page 35
 * Read [Bengio et al. 2003: A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

### Optional

There are two topics you could introduce next week. This would count toward your in-class participation grade. If you want to present one of them, announce this on this week's forum on Aula Global. In this way others will know that the topic is already taken. 

  1. Explain how neural networks are trained at a high-level (10-15 minutes). In your explanation, go through the following concepts: *loss*, *learning rate*, *(mini)batch*, *train/test/validation-split*. You can use the "Zero to Hero" content for this week's session to help your exposition. This counts for triple credit.
  2. Explain what a language model is based on the model in Fig. 1 in [Bengio et al. 2003: A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) (5-10 minutes).
   
