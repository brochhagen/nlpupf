{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece2f3b5",
   "metadata": {},
   "source": [
    "# Annotation & data\n",
    "\n",
    "NLP practicioners spend most of their time on data manipulation; data quality; and data annotation. After all, that's where most gains (and errors) stem from, compared to marginal benefits of one algorithm over another. It doesn't matter how good your algorithm is if you are evaluating it on the wrong task or on the wrong data.\n",
    "\n",
    "There is dire need for experts who understand language; how to elicit it from annotators; and how to evaluate what models (don't) learn from it.\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Why is quality human annotation hard? Where can things go wrong?</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b>Terminology.</b> Explain the following terms, in your own words and with examples:\n",
    "\n",
    "1. Sample\n",
    "2. Representative sample\n",
    "3. Biased sample\n",
    "4. Structured/Tiered sample\n",
    "5. Distribution\n",
    "</div>\n",
    "\n",
    "\n",
    "## Active learning\n",
    "\n",
    "Active learning refers to dynamic techniques to identify and label/correct new data for a machine learning algorithm.\n",
    "\n",
    "\n",
    "![](human-in-the-loop.png)\n",
    "<center>Figure 1.1 from Monarch's Human-in-the-Loop Machine Learning</center>\n",
    "\n",
    "\n",
    "### Uncertainty sampling\n",
    "\n",
    "Sample from cases in which the model is uncertain. For instance, its posterior prediction for label A is 45% and that for label B is 55%\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Give an example of a linguistic phenomenon where  uncertainty sampling is important to check model performance.</div>\n",
    "\n",
    "\n",
    "### Diversity sampling\n",
    "\n",
    "Sample from underrepresented cases. For instance, label A only applies to 1% of the data, whereas B and C take the lion's share with 49% and 50%, respectively. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Give an example of a linguistic phenomenon where  diversity sampling is important to check model performance.</div>\n",
    "\n",
    "\n",
    "### Random sampling\n",
    "\n",
    "Sample at random\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> Give an example of a linguistic phenomenon where  random sampling is important to check model performance.</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Discussion.</b> How do you decide how (in)frequently to train and evaluate your model and go through the active learning cycle?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a101be7",
   "metadata": {},
   "source": [
    "# Before learning: Train/Test split\n",
    "\n",
    "* Before training a model, you should split the data that you will evaluate your model against from the training data\n",
    "* Keep in mind that such held-out-data is not truly random\n",
    "* Keep in mind that not training on all the available data is suboptimal and, in certain contexts, avoidable\n",
    "* If the data is not truly random and likely biased, consider making the split representative or tiered\n",
    "\n",
    "\n",
    "### Practical example: Predicting average pitch (in Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3886044c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "lm(formula = pitch ~ 1, data = df)\n",
      "\n",
      "Coefficients:\n",
      "(Intercept)  \n",
      "      190.9  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "set.seed(123) #random seed\n",
    "\n",
    "n_fem <- 24 #number of female students in class\n",
    "n_mal <- 5 #number of male students in class\n",
    "\n",
    "#sample n_fem pitches from a Normal distribution with a mean of 210 and a sd of 20\n",
    "pitch_fem <- rnorm(mean = 210,\n",
    "                   sd   = 20,\n",
    "                   n    = n_fem)\n",
    "\n",
    "#sample n_mal pitches from a Normal distribution with a mean of 110 and a sd of 20\n",
    "pitch_mal <- rnorm(mean = 110,\n",
    "                   sd   = 20,\n",
    "                   n    = n_mal)\n",
    "\n",
    "#Data wrangling to get everything into a dataframe\n",
    "pitch <- c(pitch_fem, pitch_mal)\n",
    "gen  <- c(rep('F', n_fem),\n",
    "          rep('M' ,n_mal))\n",
    "df   <- data.frame(pitch = pitch, gender = gen)\n",
    "\n",
    "#univariate linear regression with no predictors \n",
    "m_avg_pitch <- lm(data    = df,\n",
    "                  formula = pitch ~ 1)\n",
    "\n",
    "print(m_avg_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8fb66",
   "metadata": {},
   "source": [
    "# Disaster labeling\n",
    "\n",
    "We will go through a practical demonstration of annotation, active learning, and machine learning.\n",
    "\n",
    "You can do execute the code either on your local machine or through a Colab. \n",
    "\n",
    "To run it on your local machine, first make sure you have `PyTorch` installed, then:\n",
    "\n",
    "  * `git clone https://github.com/rmunro/pytorch_active_learning`\n",
    "  * `cd pytorch_active_learning`\n",
    "  * `python active_learning_basics.py`\n",
    "\n",
    "\n",
    "To run it on Google's machines: \n",
    "\n",
    "  * Open a new Colab notebook: [https://colab.research.google.com/#create=true](https://colab.research.google.com/#create=true)\n",
    "  * Create a cell with: `!git clone https://github.com/rmunro/pytorch_active_learning`\n",
    "  * Create a cell with: `%cd pytorch_active_learning`\n",
    "  * Create a cell with: `!python active_learning_basics.py`\n",
    "  \n",
    "You will go through the following cycle:\n",
    "\n",
    "![](human-in-the-loop2.png)\n",
    "<center>Figure 2.2 from Monarch's Human-in-the-Loop Machine Learning</center>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Activity.</b> Go through 3-5 cycles. Each time you finish annotating a cycle, press \"s\" to save your data and see how well your model is now doing.\n",
    "\n",
    "We will discuss your impressions (what data did you get? how did your model perform?) after each cycle. While you wait for your colleages, go through the script you are running to understand the cycle under the hood (or rest; annotation is hard work).</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b>Discussion.</b>What is the random/uncertain/diversity split?  Does it make sense? Can you think of a case where a different split would be called for?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c35d24",
   "metadata": {},
   "source": [
    "# Rating the raters\n",
    "\n",
    "### Inter-annotator agreement\n",
    "\n",
    "Cohen's $\\kappa$\n",
    "\n",
    "$$\\kappa = \\frac{p_0 - p_e}{1-p_e},$$\n",
    "\n",
    "where $p_0$ is the relative observed agreement between a pair of annotators and $p_e$ is the (estimated) probability of chance agreement.\n",
    "\n",
    "$$p_e = \\frac{1}{N^2} \\sum_k n_{k1}n_{k2},$$\n",
    "for $k$ categories, where $N$ is the observations to categorize and $n_{ki}$ the number of times rater $i$ predicted category $k$.\n",
    "\n",
    "For binary classifications, this reduces to:\n",
    "$$\\kappa = \\frac{2 (TP \\times TN - FN \\times FP)}{(TP+FP) \\times (FP +TN) + (TP+FN) \\times (FN + TN)}$$\n",
    "\n",
    "If the annotators are in full agreement: $\\kappa = 1$. If there is no agreement (other than what would be expected by chance) then $\\kappa \\leq 0$.\n",
    "\n",
    "\n",
    "\n",
    "Other metrics:\n",
    "  1. Fleiss' $\\kappa$\n",
    "  2. Bangdiwala's B\n",
    "  3. Correlation coefficients\n",
    "  4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9eb5cb",
   "metadata": {},
   "source": [
    "# Further topics\n",
    "\n",
    "* Building an interface\n",
    "* Deep-dive into sampling techniques\n",
    "* Backups and databases\n",
    "* Crowdsourcing platforms, e.g., prolific and MT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpupf",
   "language": "python",
   "name": "nlpupf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
