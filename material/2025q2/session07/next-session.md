---
output:
  pdf_document: default
urlcolor: blue
---
# NLP session 07: Preparation for next session

 * Read [Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/12.pdf)'s Chapter 12, Sections 12.1 to and including 12.5: "[Model Alignment, Prompting,
and In-Context Learning](https://web.stanford.edu/~jurafsky/slp3/12.pdf)"

### Recommended:
  * Read the paper [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922)
  * Read the paper [AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/pdf/2010.15980)

### Optional

There are four topics you could introduce next week. This would count toward your in-class participation grade. If you want to present one of them, announce this on this week's forum on Aula Global. In this way others will know that the topic is already taken. 

  1. Give an introduction to *Prompting*, based on Jurafsky & Martin's Chapter 12 (from the beginning of the chapter to the end of 12.1). This counts as two in-class participation grades.
  
  2. Explain how prompts can be improved with *Model alignment* (instruction tuning; chain of thought prompting, prompt optimization), based on Jurafsky & Martin's Chapter 12 (12.2-12.5). This counts as two in-class participation grades

  3. Read the paper [AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/pdf/2010.15980) and explain it at a high level (10-15 minutes). This counts for all your in class participation.

  4. Read the paper [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922) and explain it at a high level (10-15 minutes). This counts for three in-class participation grades.